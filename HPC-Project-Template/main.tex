\documentclass[conference]{IEEEtran}
\usepackage{amsmath,amssymb,graphicx}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
\lstset{
      language=C++,
      backgroundcolor=\color{black!5}, % set backgroundcolor
      aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle=\fontsize{9}{12}\ttfamily,
  numbers=none,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{blue},
  commentstyle=\color{ForestGreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
  }

% nice symbols for real and complex numbers
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}
% bold paragraph titles
\newcommand{\mypar}[1]{{\bf #1.}}

\begin{document}

\title{Evaluating tensor cores}


\author{\IEEEauthorblockN{John Carlsson, Cyprien Toulon de Courtex}
\IEEEauthorblockA{Department of Computer Science\\
 University of Salerno\\
 Italy}
}

\maketitle

\begin{abstract}\label{sec:abstract}
This project was aimed at evaluating tensor cores in cuda through experiemnts to get a better understanding off
high performance computing through different methods. With the ultimate goal to compare the time to calculate a
matrix multiplication addition. At the start of the project, we were fairly certain that the fastest algorithm would be 
the tensor cores, followed by Cuda with shared memory,Cuda global memory, and at the bottom, an optimized CPU version. However, this was not the case.
The results show that Cuda shared memory is the fastest with global memory just behind. For small matrices tensors appear superior.
\end{abstract}

\section{Introduction}\label{sec:intro}


Over the past decade, the increasing demand for deep learning and AI applications has
fueled the need for highly efficient tensor operations. Traditional GPU architectures are powerful but 
are often limited in their ability to fully exploit the potential of 
tensor computations due to the reliance on higher-precision floating-point arithmetic. 
Tensor cores address this limitation by leveraging mixed-precision arithmetic, combining 
high throughput with reduced precision.

Tensor cores, introduced in NVIDIA's Volta architecture and further enhanced in 
subsequent architectures such as Turing and Ampere, have revolutionized the performance of 
tensor computations on GPUs. These specialized hardware units offer significant speedups 
by providing native support for low-precision tensor operations, 
specifically matrix multiplications and convolutions. 
The purpose of this project report is to present an evaluation of tensor cores,
exploring their capabilities compared to CPU and GPU based computation.

In this project, we aim to evaluate tensor cores using the Matrix Multiplication Accumulate (MMA) with various 
workloads and compare the results to the CPU and GPU performances. 

By conducting this comprehensive evaluation, we aim to provide insights into the capabilities and 
limitations of tensor cores. Additionally, the findings from this study will contribute to the 
broader understanding of GPU acceleration techniques for tensor operations and their potential 
impact on high-performance computing.

\subsection{Motivation}\label{sec:Motivation}

The MMA operation plays a critical role in accelerating various computational tasks.
It involves three matrices and is fundamental within many computational domains, 
such as deep learning, scientific simulations, and image processing. However, performing MMA 
efficiently and at scale is difficult due to the intensity of the operation. 

The difficulties that arise when trying to implement a fast version of MMA are due to 
a combination of factors including memory access patterns, data dependencies, 
computational intensity, matrix sizes, hardware limitations, and optimization trade-offs.

The objective of this paper is to provide valuable insights for optimizing MMA computations
and facilitate the adoption of tensor cores as a powerful tool for accelerating MMA 
operations in comparison to CPU and GPU. Our contributions will be as follows:
\begin{itemize}
  \item We will measure the speedup achieved by tensor cores measured in actual seconds and explore how 
  performance scales.
  with the size of the matrices.

  \item We will compare the performance of tensor cores with CPU and GPU implementations of MMA 
  using different matrix sizes and precision settings.
  
  \item We will analyze the impact of data precision on the performance of tensor cores and 
  investigate the trade-offs between accuracy and computation time.
  
  \item We will explore optimization techniques for maximizing the utilization of tensor cores 
  and achieving optimal performance.
  
  \item We will provide insights into the limitations and challenges of tensor cores, 
  including memory constraints, data dependencies, and scalability issues.
  
  \end{itemize}
  
  \section{Background}\label{sec:background}
  
  \subsection{Tensor cores}\label{sec:tensor-cores}
  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.32]{figures/SM.png}
    \caption{SM architecture.\cite{NVIDIA_Tensor_Core_Programmability_KTH}}
    \label{fig:performance-comparison}
  \end{figure}

  tensor cores are specialized hardware units introduced in NVIDIA GPUs that are designed to accelerate matrix computations, 
  specifically matrix multiplications and convolutions. They leverage mixed-precision arithmetic, 
  combining high throughput with reduced precision to achieve significant speedups in tensor operations.
  
  The key feature of tensor cores is their ability to perform mixed-precision operations using 
  half-precision floating-point (FP16) data types. By using FP16, tensor cores can process a larger 
  number of elements simultaneously, leading to improved throughput. 
  
  tensor cores operate on up to 16x16 matrix tiles, performing matrix multiplication and accumulation (MMA) 
  operations. These operations are highly parallel and can be efficiently executed on tensor cores. 
  By exploiting the parallelism of tensor cores and their ability to process multiple elements 
  simultaneously, significant performance gains can be achieved compared to traditional GPU 
  implementations.

  The tensor operations are warp wize, this means that each tensor instructions must work on the same 
  data within the same warp.
  
  \subsection{Matrix Multiplication Accumulate (MMA)}\label{sec:mma}
  
  Matrix Multiplication Accumulate (MMA) is a key operation in many computational tasks, including deep 
  learning, scientific simulations, and image processing. It involves three matrices: two input matrices 
  (A and B) and an output matrix (C). The operation computes the matrix product of A and B and accumulates 
  the result into C.
  
  Mathematically, the MMA operation can be defined as follows:
  
  \[ C \leftarrow \alpha \cdot A \cdot B + \beta \cdot C \]
  
  where A is an m x k matrix, B is a k x N matrix, C is an m x N matrix, and $\alpha$ and $\beta$ are scalar coefficients.
  
  The MMA operation is computationally intensive and can benefit greatly from hardware acceleration. 
  tensor cores, with their ability to efficiently perform MMA operations, offer a promising solution 
  for accelerating this operation.
  
  \section{Experimental Setup}\label{sec:experimental-setup}
  
  In this section, we describe the experimental setup used to evaluate 
  tensor cores and compare their performance with CPU and GPU implementations of MMA.
  We did a multitude of tests with different parameters to see how performance scale over matrix size
  and how conducting many runs consequently affects performance over time.

  We used square matrices with the sizes of 32, 64, 128, 256, 512, 1024, 2048, 4096 and 8192.

  we conducted single comparison runs and then multiple runs to get a better average for time and error.
  
  To understand the effect of each optimization methods, we produced different implementations, exploiting 
  Cuda, shared memory, tensor cores, etc.

  \subsection{Hardware Configuration}\label{sec:hardware-configuration}
  
  The experiments were conducted on a system equipped with an NVIDIA GPU that supports tensor cores.
  The specific GPU model is seen below. The GPU belongs to a group of compute capability 7.5. 
  
  \begin{table}[htbp]
  \caption{GPU Specifications\cite{Voltatuningguide}}
  \centering
    \begin{tabular}{|c|c|}
    \hline
    GPU Model & NVIDIA Quadro 4000 RTX \\
    \hline
    Streamlline Multiprocessors (SM) & 4 \\
    \hline
    Compute capability & 7.5 \\
    \hline
    
  \end{tabular}
  \end{table}
  
  \subsection{Software Configuration}\label{sec:software-configuration}
  
  The experiments were conducted using the following software tools and frameworks:
  
  \begin{itemize}
    \item CUDA Toolkit version 11.8
    \item WMMA
    \item C++17 compiled with g++ 11.3.0
  \end{itemize}
  
  We implemented the MMA operation in different ways: CPU, GPU (without tensor cores), and GPU (with tensor cores). 
  The CPU implementation was written in C++, while the GPU implementations were developed using CUDA.

  
  \section{Code}
  In this section we present the main part of the code. The code shown is the computational part. Since this is
  deemed most important. We show here the Cuda code and the tensor code.

  \subsection{CPU code}\label{sec:CPUCode}
  We implement a CPU version of the MMA operation to have a reference to verify our computation results.
  The CPU code is an implement of the MMA operation that uses the following optimizations:
  \begin{enumerate}
    \item tiling
    \item parallelisation using OpenMP
    \item compiler vectorisation for AVX512 using OpenMP
    \item compiler optimization flags
  \end{enumerate}


  \subsection{Cuda code}\label{sec:CudaCode}
  The code is heavily simplified to keep the essence. For full code, see appendix.

  \begin{lstlisting}
    __global__ void mat_mul_add(Out *out, In *A, In *B, Out *c, int n)
    {   
        sum_acc = c;
        for (tileNum = 0; tileNum < n/TILE_SIZE; tileNum++)
        {   // load into shared memory, coalesced
            a_shared = A;
            b_shared = B;
            __syncthreads();
            for (int k = 0; k < TILE_SIZE; k++)
            {sum_acc += a_shared[threadIdx.y][k] * b_shared[k][threadIdx.x]; }
        }
        out = sum_acc;
    }   
  \end{lstlisting}
  
  
  \subsection{Tensor code}\label{sec:TensorCode}
  To make the code more compact, these simplifications are made:\\
  Bs = Blocksize \\
  tIdx = threadIdx \\
  bIdx = blockIdxx \\
  bDim = blockDimx

  \begin{lstlisting}
__global__ void mat_mul_add_tensor(half *a, half *b, float *C, float *d, int N){
// Declare the fragments
wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;
wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> b_frag;
wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;
wmma::load_matrix_sync(c_frag, C, N, wmma::mem_row_major);
// Loop over k
for (int i = 0; i < N; i += WMMA_K){
  wmma::load_matrix_sync(a_frag, a, N);
  wmma::load_matrix_sync(b_frag, b, N);
  // Perform the matrix multiplication
  wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);}
  wmma::store_matrix_sync(d, c_frag, N, wmma::mem_row_major);}
  \end{lstlisting}


  \section{Benchmarking Methodology}\label{sec:benchmarking-methodology}
  
  To evaluate the performance of tensor cores, we performed a series of experiments using different 
  matrix sizes and precision settings. For each experiment, we measured the execution time of the 
  MMA operation for CPU, GPU (without tensor cores), and GPU (with tensor cores) implementations.
  
  We varied the matrix sizes from small to large to analyze the impact of matrix dimensions on 
  performance. Additionally, we tested different precision settings, including full precision (FP32)
  and mixed precision (FP16).
  
  To ensure accurate measurements, we repeated each experiment multiple times and calculated 
  the average execution time. We also recorded the peak memory usage during each experiment to 
  analyze the memory requirements of tensor cores.
  
  \section{Results and Analysis}\label{sec:results-analysis}
  
  In this section, we present the results of our experiments and analyze the performance of 
  tensor cores compared to CPU and GPU implementations of MMA.

  % insert a bunch of graphs
  
  \subsection{Performance Comparison}\label{sec:performance-comparison}

  %Figure~\ref{fig:performance-comparison} 
  Insert Picture here\\
  shows the execution time of the MMA operation for different matrix sizes and precision settings.
  
  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{figures/Comp_time.png}
    \caption{Performance Comparison}
    \label{fig:performance-comparison}
  \end{figure}

  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{figures/Tot_time_deac.png}
    \caption{Performance Comparison total time with deallocation}
    \label{fig:time-comparison}
  \end{figure}



  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.38]{figures/Relative speed up to Cuda.png}
    \caption{Relative speedup to CUDA with global memory}
    \label{fig:performance-comparison}
  \end{figure}
  
  ANALYSIS
  
  \subsection{Impact of Precision}\label{sec:impact-precision}
  

  Next, we analyze the impact of precision on the performance of tensor cores. Figure %\ref{fig:precision-impact} illustrates the execution time of the MMA operation for different precision settings using tensor cores.
  
  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.42]{figures/Mean of error compared to CPU.png}
    \caption{Impact of Precision on tensor cores Performance}
    \label{fig:precision-impact}
  \end{figure}
  
  The results show that using mixed precision (FP16) with tensor cores can provide significant performance improvements compared to full precision (FP32).
  This is because tensor cores can process a larger number of elements simultaneously in FP16,
  leading to higher throughput. This accuracy also improves with time for the matrix sizes.
  Using FP16 then introduces a degree of numerical approximation, affecting the accuracy of the computation.
  Therefore, the choice of computing should be carefully considered based on the specific
  requirements of the application.
  
  \subsection{Optimization Techniques}\label{sec:optimization-techniques}
  
  To maximize the utilization of our approaches and achieve optimal performance, we explored several 
  optimization techniques, including:
  
  \begin{itemize}
    \item Matrix tiling: By partitioning the matrices
    into smaller tiles that fit the tensor Core dimensions, we can improve data locality 
    and exploit the parallelism of tensor cores more effectively.
    \item Memory access patterns: Optimizing memory access patterns can minimize data 
    dependencies and ensure efficient memory utilization, reducing memory stalls and improving 
    performance.
    
  \end{itemize}
  
  \section{Limitations and Challenges}\label{sec:limitations-challenges}
  The main limitations faced in this project was the limited information regarding how to program tensor cores.
  The cublass library, which is an optimized version of MMA with tensor cores, developed by NVIDIA is not open source,
  so there is no real way of obtaining it. This led to experimenting with different setups to achieve the fastest copmutation possible.
  Another challenge to achieve fast computation for larger matrices with tensors are the fact that the GPU 
  only has 64Kb of memory for each work group. With the tensors, the sum of shared memory is then N*N *(16+16).
  Which corresponds to the matrix size, and the byte size of the elements in the A and B matrix. This entails that the 
  largest matrices that can fit this is of 32 x 32 elements of half precision.


  \section{Conclusion}\label{sec:conclusion}
  
  In this paper, we have explored the performance of NVIDIA tensor cores for matrix multiplication and accumulation operations. 
  We conducted experiments to compare the performance of tensor cores with CPU and GPU implementations, 
  considering different matrix sizes and precision settings.

  From our result we can see that tensor cores without any optimization have significantly better computational power
  than the other, optimized approaches. It is then our belief that to optimize the tensor core version would 
  decrease the computational time further. The total time comparison shows that allocation of memory has a significant
  impact on the time from input to output. Therefore using tensor cores is optimal in problen where the same data is
  handled continously, for example in convulational neural network and image processing. The error seems to decrease
  with matrix size which can be attributed to what values we are using when testing. Using only integers, the error is 0.

  

\bibliographystyle{IEEEtran}
\bibliography{sources}

\end{document}